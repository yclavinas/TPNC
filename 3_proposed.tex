\section{Proposed Changes}
\label{sec:adaptations}

In this work, we propose three improvements to the GAModel: A reduced
genome representation, Hybridisation with the ETAS empirical model,
and the clustering of the earthquake catalog. Each of these changes
are described below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
\subsection{Reduced Genome Representation}

In the GAModel, problem is represented as a vector $X$ where each bin
corresponds to an element in the vector. As the number of bins in a
region numbers into the thousands, this representation leads to a huge
search space to be explored.

We have observed that in many cases, the vector of catalog
earthquakes is sparse. In other words, most of the elements of $X$
will be zero or close to it. To use this fact to decrease the
search space, we propose a ``reduced'' representation of a risk Model.

A summary of the reduced representation is as follows: First, before
initialising the Genetic Algorithm, we estimate the expected total
number of earthquakes in the model based on the past data. Then, this
value is used as the total number of earthquakes to be added to the
model. The reduced representation will be a vector of bin coordinates
for each of these earthquakes, representing their position inside the
target area. This is much smaller than a representation including each
bin as an element.


\subsubsection*{Implementation}

The reduced representation is a vector $V$ of ordered pairs. The first
element of this pair is the integer index that identify a bin in the
model. The second element of the pair is the number of earthquake
occurrences estimated for this bin.

The size of the vector $V$ is calculated as the number of bins in the
historical catalog that contain at least one earthquake. For each
element in $V$, the bin index and the estimated number of occurrences
are drawn randomly from a uniform distribution.

To generate a model from the reduced representation, we need to go two
intermediate steps. The first one is to transform the reduced
representation into a regular representation. This is achieved by
copying the estimated value of an element to the bin indicated by
stored index for that element. Bins that are not indicated by any
element in the vector are set to zero estimated earthquakes.

The second step is to apply the inverse Poisson on the estimated
values to retrieve the number of earthquakes, as described in
algorithm~\ref{inversePoisson}.

\subsubsection*{Operators}

The reduced representation can use the same one point crossover as the
GAModel, but a different mutation operation is required. The mutation
operator works by selecting one element in the vector $V$, and drawing
new values for the index and the estimation parameter from a uniform
distribution.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hybridisation with ETAS}

% Motivation
The GAModel produces risk models without using any sort of domain
knowledge, other than the difference between the individual being
evaluated and the earthquake catalog data.

However, one simple observation that could be added to the GAModel is
that earthquakes cluster in space and time. Large earthquakes are
usually followed by a wave of smaller earthquakes, these pairings
being commonly known as \emph{mainshocks} and
\emph{aftershocks}~\cite{schorlemmer2010first}.

To include this idea into the GAModel, we modify the process which
generates a Model from an individual. In this modified process, one
individual will only produce mainshocks into the model, afterwards the
aftershock are derived from the mainshocks, using empirical seismic laws
such as the \emph{modified Omori Law}.

We define this hybridisation between empirical seismic laws and the
GAModel as the \emph{EMP-GA}. Below, we detail the implementation of
both steps.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Implementation}

The EMP-GA generates models with mainshocks and aftershocks following
a two-step procedure.

In the first step, we use the GAModel to generate a set of mainshock
earthquakes, which we will refer to as \emph{synthetic mainshock
  data}. In the second step, we use seismic empirical equations to
obtain the aftershocks from the synthetic mainshock data, and add them
to the model.

The process we use to generate aftershocks from the synthetic
mainshock data is inspired by the space-time epidemic-type aftershock
sequence (ETAS). The total number of earthquakes in a bin is given as
% TODO: Sentar com o Yuri e pedir para ele me explicar a
% implementacao/razao desta equacao detalhe por detalhe.
\begin{equation}\label{emp-model}
 \Lambda(t,x,y|\Upsilon_t) = [\mu(x,y) + \displaystyle\sum_{t_i \in t}
   K(M_i)g(t-t_i)P(x,y)]J(M).
\end{equation}
In this equation, $t$ is the target time for the model, $x,y$ are
latitude/longitude coordinates within the target area, $\Upsilon_t$ is
the number of mainshocks derived in the first step, $\mu(x,y)$ is the
expected number of earthquakes at the $(x,y)$ bin, $t_i$ is a time
interval within $t$, $K(M_i)$ is the total amount of triggered events,
$g(\Delta t)$ is the probability density form of the modified Omori
law, $P(x,y)$ is a function that distributes aftershocks in space
nearby the mainshock, and $J(M)$ is the ETAS simulated magnitude. Let
us explain each of these components below.

\subsubsection*{Omori's Law and Triggered Events}

The Omori law, which is considered to be an empirical seismic formula
which has withstood the test of
time~\cite{utsu1995centenary,omori1895after}, is a power law that
relates the magnitude of an earthquake with the decay of aftershock
activity over time. It can estimate the number of aftershocks based on
the mainshocks in the synthetic data generated by one individual in
the EMPGA. For this approach, we use the probability density function
(PDF) form of the modified Omori law~\cite{zhuang2004analyzing},
defined as

\begin{equation}\label{omori}
  g(\Delta t)= \frac{(p-1)}{c(1+ \frac{t}{c})^{-p}}.
\end{equation}

In this equation, $p$ and $c$ are
constants. Utsu~\cite{utsu1995centenary}, summarise the studies of
this formula for the Japan case, and describe a range for these
variables using the Davidon-Fletcher-Powell optimisation
procedure. These ranges, used in ETAS, are 0.9 to 1.4 for $p$, and
0.003 and 0.3 for $c$.

Also, $\Delta t$ is the time interval for how long a mainshock may
influence or cause an aftershock. According to
Yamanaka~\cite{yamanaka1990scaling}, this value must be chosen
carefully, for a value too short will lead to a small number of
aftershocks, while a value too long might confound aftershocks and
background activity. His work suggests the values $p = 1.3$, $c =
0.003$, and $\Delta t = 30$ days, which we use in this paper.

The total amount of events triggered by a mainshock is represented in
equation~\ref{emp-model} as $K(M_i)$. To calculate this value,
we count the number of aftershocks within a given area $A$ from
the mainshock, using the formula

\begin{equation}\label{triggered}
 K(M_i) = A\ exp([\alpha(M_i-M_c)]).
\end{equation}

Where $M_c = 3.0$ is the magnitude threshold and $\alpha(M)$ is defined
as the inverse of the magnitude, according to
Ogata~\cite{ogata2006space}. The area $A$ is obtained using the
equation from Yamanaka~\cite{yamanaka1990scaling}

\begin{equation}
A = e^{(1.02M -4)}.
\end{equation}

Using the number of triggered events per magnitude $K(Mi)$, and the
Modified Omori PDF $g(t)$, it is possible to calculate the total
number of earthquakes generated from a mainshock, by iterating
over $t_i$:
\begin{equation}
\displaystyle\sum_{t_i \in t} K(M_i)g(t-t_i)
\end{equation}

% TODO: From this description, it does not seem that P(x,y) is actually
% part of equation emp-model: double check with Yuri.
The resulting aftershocks need to be spread on bins near the mainshock
position. The $P(x,y)$ component of equation~\ref{emp-model} fills
this role. It calculates the position of the aftershocks based on the
position of the original mainshock. It simply places each aftershock
either north, south, east or west of the mainshock, getting further
from the origin after each iteration, until there are no more events
to be placed.

%% This equation is not clear -- where is the component that place
%% Aftershocks further away from the origin?
%\begin{subequations}
%\begin{gather*}
%        model[x+y] = (aftershocks-[model[x]-2*x])/4;\\
%        model[x-y] = (aftershocks-[model[x]-2*x])/4;\\
%        model[x-y*row] = (aftershocks-[model[x]-2*x])/4;\\
%        model[x+y*row] = (aftershocks-[model[x]-2*x])/4
%\end{gather*}
%\end{subequations}

% TODO: but WHAT is J(M)?
Finally, $J(M)$ is obtained by using the function \emph{etasim}, from
the SAPP \textit{R} package~\cite{webSapp} that simulates magnitude by
Gutenberg-Richterâ€™s Law.

The above equations are put together in algorithm~\ref{algoEquations}.

\begin{algorithm}[H]\label{algoEquations}
  \caption{Aftershock distribution from empirical laws}
  \begin{algorithmic}
    \STATE FOR EACH BIN:
    \IF {Number of earthquakes in bin > 12} % Why?
    \STATE {Reduce number of earthquakes in bin to 12}
    \ENDIF
    \STATE aftershocks = 0
    \STATE magnitude values for earthquakes in bin = J(M)
    % What does this mean?
    % \STATE model  = attributeMagnitudeToEarthquake(model, J(M) )
    % \STATE magnitudes = getMagnitudeMainshock(model)
    
    \FOR{magnitude in magnitudes} 
    \FOR{t in time} 
    \STATE aftershocks += g(t)*K(magnitude)
    \ENDFOR
    \ENDFOR
    % Need a more detailed algorithm for P(x,y)
    \STATE Use P(x,y) to distribute aftershocks to neighbour bins
  \end{algorithmic}
\end{algorithm}

% TODO: Probably needs a longer explanation of ETAS in the
% Bibliography section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsection{Clustering the Catalog Data}

The third adaptation to the GAModel that we study in this paper is the
clustering of the earthquake catalog data using Spectral
Clustering. Unlike the two adaptations described beforehand, this one
does not require any change on the algorithm itself, happening instead
as a data pre-processing step when building the model. After the
catalog data is pre-processed, the Genetic Algorithm is applied
normally, using the de-clustered data for fitness evaluation.

This pre-processing step aims to remove redundant information from the
catalog, by clustering together earthquakes which are closely related
in a mainshock/aftershock relationship~\cite{van2012seismicity}.

However, because it is difficult to determine exactly when two
earthquakes should be clustered together, we choose a non-supervised
method, Spectral Clustering, to generate the clusters.

Spectral Clustering involves constructing a similarity matrix of the
elements to be clustered, finding the k-Nearest-Neighbours graph (KNN)
based on the similarity matrix, calculating the Laplacian matrix of
the KNN graph, and performing k-means clustering on the eigenvectors
of this matrix.

One of the main characteristics of Spectral Clustering that make it
interesting for this problem is that it can be very computationally
efficient~\cite{Ye2016}. This is very important for the clustering of
earthquake data, since each data set can contain tens of thousands of
earthquakes.

\subsubsection*{Spectral Clustering Implementation}

Let the earthquake catalog data be represented as a vector $X = \{x_1,
x_2, \ldots, x_n\ in \Re_d\}$, where $n$ is the number of earthquakes
in the catalog, $d$ is the number of attributes that characterise an
earthquake in the catalog, and $K$ is the desired number of
clusters. The clusters are calculated following
algorithm~\ref{spectralclustering}

% TODO: Probably needs to include how to calculate KNN
\begin{algorithm}[H]\label{spectralclustering}
  \caption{Spectral Clustering}
  \begin{algorithmic}
    \STATE{Construct the similarity matrix S}
    \FOR{i in $X$}
    \FOR{j in $X$}
    \IF{$i$ and $j$ are connected in the KNN graph}
    \STATE{$s_{i,j} = \exp(-||x_i-x_j||^2/2\sigma^2$}
    \ELSE
    \STATE{$s_{i,j} = 0$}
    \ENDIF
    \ENDFOR
    \ENDFOR
    \STATE Matrix $D = n x n$ diagonal matrix where $d_{i,i} = \sum^n_{j=1}s_{ij}$
    \STATE Compute Matrix $L = D - S$
    \STATE Compute $K$ smallest eigenvectors of $L$
    \STATE Compute matrix $V = (v_{ij})_{nxK}$, using these eigenvactors as columns.
    \STATE Compute matrix $U = (u_{ij})_{nxK}$, normalising the rows
    of $V$ such as $u_{i,j} = v_{i,j}/\sqrt{\sum_jv^2_{ij}}$
    \STATE Let each row in $U$ represent a data point, and cluster
    these points using k-means
    \FOR{each point $x_i$ in $X$}
    \STATE Assign the cluster of $u_i$ to $x_i$
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

In this algorithm, $||x_i-x_j||$ is the Euclidean distance between
data points $x_i,x_j$. We use the number of nearest neighbours equal to
five, and $\sigma$, the kernel parameter, equal to $100$.

% TODO: Minutes is divided by 1000 to make the values smaller
We cluster the earthquakes based on their latitude, longitude, time
(in minutes), and depth. By observing the distributions of the
eigenvectors, we defined the weight of each dimension in the algorithm:

\begin{itemize}
\item latitude and longitude: 150
\item time: 7
\item Depth: 0.5
\end{itemize}
